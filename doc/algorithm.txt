
Goal: Maximize consistency of the language within a document.

To do so we use an n-gram-based language model.

We don't want to be too heavy-handed in our language model though;
we want to incorporate local language use as well.

We begin with a pre-fabricated sourced from an external "global" corpus,
in this case we use Google Books' 3-ary n-grams.

Upon initialization we incorporate the "local" corpus of documents into our
language model, likely by parsing documents in the current and parent folders.

It is this local model we should use first against new documents. This
allows our checker to tailor its behavior to its environment, whether the
documents are legal documents, school book reports, bad sci-fi novels, etc.

http://en.wikipedia.org/wiki/Text_corpus
http://en.wikipedia.org/wiki/Language_model#N-gram_models
http://en.wikipedia.org/wiki/N-gram
http://ngrams.googlelabs.com/datasets

overhere -> overhear (x -> x')
over,here -> over,here (x,y -> x,y)
over,hear -> overhear (x,y -> x')
i,now,the -> i,know,the (x,y,z -> x,y',z)
than,you,very,much -> thank,you,very,much (x,y,z,zz -> x',y,z,zz)
thank,yo -> thank,you (x,y -> x,y')

Consider:
	fingerprinting words by content: hello = e:1,h:1,l:2,o:1

Algorithm:
	AutoRevise(doc):
		Target the smallest, least-known ngrams first.
		List alternatives
			Begin with cheap, straight-forward, common alternatives and progress to more expensive/complex iff necessary
				Try to solve individual, unknown tokens first
				Preserve token boundaries (cheap)
					Edit distance 1, edit distance 2
					Phonetic similarities
				Disregard token boundaries (expensive)
					Parse all possible token sequences
						
			For each alternative
				Score its effectiveness by evaluating the complete repercussions
		Retain the best alternatives
		Propose revisions unobtrusively.
			Never modify without the user's permission. http://en.wikipedia.org/wiki/Cupertino_effect
		Record revision selection.
			Incorporate into future decisions.
		If revision is selected:
			Update document and all statistics/ngrams to reflect the change

parse/load base corpus of target language
parse/load local corpus

calculate frequency of all ngrams 1..n
sort ngrams on size:asc, freq:asc
for ng in ngrams below some threshold:
	calculate feasible permutations for ng
		note: focus only on one area at a time, as the resulting change will modify the rest of the document
		for tok in ng:
			calculate list of permutations: spelling edits, pronunciation
		account for merging/splitting of tokens, etc.


conduct re search : conduct research
hitherehowareyou : hi there how are you



